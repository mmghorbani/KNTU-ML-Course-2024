# -*- coding: utf-8 -*-
"""SWaT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xPmVBYntZG-P0gESfOhUO8nXEANdTmto
"""
"""
# Cyber-attack detection on SWaT dataset
K.N. Toosi Machine Learning Course - 2024
Instructor: Dr. Mahdi Aliyari
Autor: Mohammad Mahdi Ghorbani
"""

## Preprocess data

### Import dataset

!pip install lightgbm

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sb
from scipy.stats import skew, kurtosis, zscore
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV
from sklearn.svm import SVC
from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.decomposition import PCA
from numpy.linalg import eig
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import lightgbm as lgb
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import time

!pip install --upgrade --no-cach-dir gdown
!gdown 1ZCvW_wVmgJhjZQK1nhDTOAxliNPJR0IH # Attack_v0
!gdown 1okbG5Y171GKNbdUrtOTbpUFHRXapfGvM # Normal_v0

"""Attack Dataset"""

data_attack = pd.read_csv('/content/SWaT_Dataset_Attack_v0.csv').set_index(' Timestamp')

data_attack

data_attack.columns[:]

# Rename target vector
data_attack = data_attack.rename(columns={'Normal/Attack': 'target'})

# Remove space in columns names
new_column_names = [column.split(" ")[-1] for column in data_attack.columns]
data_attack.columns = new_column_names
data_attack.columns

# Label endocing and typographical error
data_attack['target'] = data_attack['target'].replace({'Normal': 0, 'A ttack': 1, 'Attack': 1})
# data_attack['target'] = data_attack['target'].replace({'A ttack': 'Attack'})

data_attack.dropna();
data_attack.isna().sum()

"""Normal data"""

data_normal = pd.read_csv('/content/SWaT_Dataset_Normal_v0.csv').set_index(' Timestamp')

data_normal

data_normal = data_normal.rename(columns={'Normal/Attack': 'target'})

data_normal.columns[:]

data_normal['target'] = data_normal['target'].replace({'Normal': 0, 'Attack': 1})

data_normal.dropna();
data_normal.isna().sum()

"""Split dataset to continuous and discrete"""

discrete_features = ['MV101','P101', 'P102', 'MV201', 'P201', 'P202', 'P203',
                     'P204', 'P205', 'P206', 'MV301', 'MV302', 'MV303','MV304',
                     'P301', 'P302', 'P401', 'P402', 'P403', 'P404', 'UV401',
                     'P501', 'P502', 'P601', 'P602', 'P603']

data_normal_cont = data_normal.drop(discrete_features, axis=1)
data_normal_cont = data_normal_cont.drop('target', axis=1)
data_normal_cont.head()

data_attack_cont = data_attack.drop(discrete_features, axis=1)
data_attack_cont = data_attack_cont.drop('target', axis=1)
data_attack_cont.head()

target_normal = data_normal['target']
target_attack = data_attack['target']

print(data_normal_cont.shape, data_attack_cont.shape)
print(target_normal.shape, target_attack.shape)

"""### Feature extraction"""

class TimeSeriesToTabular:
    def __init__(self, N=60, attack_threshold=0.1):
        """
        Initialize the converter with batch length and attack threshold.

        Parameters:
        N (int): Length of each batch (number of samples per batch).
        attack_threshold (float): Threshold for labeling a batch as 'Attack'.
        """
        self.N = N
        self.attack_threshold = attack_threshold

    def extract_features(self, batch):
        """
        Extract statistical features from a batch.

        Parameters:
        batch (ndarray): Batch of data to extract features from.

        Returns:
        list: List of extracted features.
        """
        features = []
        for col in range(batch.shape[1]):
            data = batch[:, col]
            features.extend([
                data.mean(), data.std(), data.min(), data.max(),
                np.median(data), np.percentile(data, 25),
                np.percentile(data, 75),
                skew(data, nan_policy='omit') if len(np.unique(data)) > 1 else 0,
                kurtosis(data, nan_policy='omit') if len(np.unique(data)) > 1 else 0
            ])
        return features

    def transform(self, X, y):
        """
        Transform the time series data into a tabular format.

        Parameters:
        X (ndarray): Feature matrix (time series data).
        y (ndarray): Target vector (labels).

        Returns:
        DataFrame: Tabular feature DataFrame.
        DataFrame: Tabular labels DataFrame.
        """
        X = np.array(X)
        y = np.array(y)

        M = len(X) // self.N  # Number of batches
        tabular_data = []
        batch_labels = []

        for i in range(M):
            batch_X = X[i*self.N:(i+1)*self.N, :]  # Get the i-th batch of features
            batch_y = y[i*self.N:(i+1)*self.N]  # Get the i-th batch of targets

            if batch_X.shape[0] < self.N:
                continue

            batch_features = self.extract_features(batch_X)  # Extract features
            attack_ratio = np.mean(batch_y)  # Calculate the ratio of "Attack" labels
            batch_label = 'Attack' if attack_ratio >= self.attack_threshold else 'Normal'
            tabular_data.append(batch_features)
            batch_labels.append(batch_label)

        # Create a DataFrame for features
        feature_names = [f'feature_{i}_{stat}' for i in range(X.shape[1]) for stat in
                         ['mean', 'std', 'min', 'max', 'median', '25pct', '75pct', 'skew', 'kurt']]
        features_df = pd.DataFrame(tabular_data, columns=feature_names)

        # Create a DataFrame for labels
        labels_df = pd.DataFrame(batch_labels, columns=['label'])

        return features_df, labels_df

# Initialize the converter
converter = TimeSeriesToTabular(N=30, attack_threshold=0.3)

# Transform the data
data_normal_cont_new , target_normal_new = converter.transform(data_normal_cont, target_normal)
data_atttack_cont_new , target_attack_new  = converter.transform(data_attack_cont, target_attack)

X_cont = pd.concat([data_normal_cont_new, data_atttack_cont_new], ignore_index=True)
y_cont = pd.concat([target_normal_new, target_attack_new], ignore_index=True)

print(X_cont.shape, y_cont.shape)

"""### Shuffling and Splitting data"""

SEED = 24

x_train , x_test , y_train , y_test = train_test_split(X_cont, y_cont, test_size = 0.3, random_state = SEED)
x_train.shape, x_test.shape, y_train.shape, y_test.shape

print(x_train.shape, x_test.shape)
print(y_train.shape, y_test.shape)

scaler = StandardScaler()
scaler.fit(x_train)

x_train_standardized = scaler.transform(x_train)
x_test_standardized = scaler.transform(x_test)

print(np.mean(x_train_standardized), np.mean(x_test_standardized))
print(np.std(x_train_standardized), np.std(x_test_standardized))

"""### Dimension reduction"""

def covariance(x):
    return (x.T @ x)/(x.shape[0]-1)

cov_mat = covariance(x_train_standardized) # np.cov(X_std.T)

# Eigendecomposition of covariance matrix
eig_vals, eig_vecs = eig(cov_mat)

print(eig_vals)

# Set the global DPI setting for all figures
plt.rcParams['figure.dpi'] = 200

plt.figure(figsize=(20,5))
plt.stem(eig_vals, use_line_collection = True)
plt.xlabel('Eigen value index')
plt.ylabel('Eigen value')
plt.grid()
plt.show()

pca = PCA(n_components=5, random_state=SEED)
pca.fit(x_train_standardized)

x_train_pca = pca.transform(x_train_standardized)
x_test_pca = pca.transform(x_test_standardized)

print(x_train_pca.shape, x_test_pca.shape)

"""### Oversampleing data"""

# Oversample
sub = SMOTE(
    sampling_strategy= 'auto',
    random_state=SEED,
    )

x_sub, y_sub = sub.fit_resample(x_train_pca, y_train)
print(x_sub.shape, y_sub.shape)

plt.figure(figsize=(15,5))
plt.subplot(1,2,1)
sb.countplot(data=y_train, x='label', hue='label')
plt.title('Class balance before over-sampling')

plt.subplot(1,2,2)
sb.countplot(data=y_sub, x='label', hue='label')
plt.title('Class balance after over-sampling')

"""## Classification

### SVM
"""

# Define the parameter grid
param_grid = {
    'C': [0.1, 1, 10, 100],
    'gamma': [1, 0.1, 0.01, 0.001],
    'kernel': ['rbf']
}

# Initialize the SVM model
svm = SVC(random_state=SEED)

# Set up the grid search
grid_search = GridSearchCV(
    estimator=svm,
    param_grid=param_grid,
    cv=5,
    verbose=2,
    n_jobs=-1,
    )

# Fit the model
grid_search.fit(x_sub, y_sub)

# Print the best parameters and estimator
print("Best parameters found: ", grid_search.best_params_)
print("Best estimator: ", grid_search.best_estimator_)

# Evaluate the model on the test set
start_svm = time.time()

clf_svm = SVC(kernel='rbf', C=100, gamma=1, random_state=SEED)

clf_svm.fit(x_sub, y_sub)

y_pred_svm = clf_svm.predict(x_test_pca)

clf_svm.score(x_test_pca, y_test)

end_svm = time.time()

print(f'Runtime for SVM: {round(end_svm - start_svm)} seconds')

print('SVM (kernel=RBF)')
print(classification_report(y_test, y_pred_svm))

labels = ['Attack', 'Normal']
cm_svm = confusion_matrix(y_test, y_pred_svm)
disp_svm = ConfusionMatrixDisplay(confusion_matrix=cm_svm, display_labels=labels)
disp_svm.plot(cmap='Blues')
plt.title('SVM (kernel= RBF)')

"""### LGBM"""

y_train_encoded = np.where(y_train == "Attack", 1, 0)
y_test_encoded = np.where(y_test == "Attack", 1, 0)

start_lgbm = time.time()

# Create the LightGBM dataset
train_data = lgb.Dataset(x_train, label=y_train_encoded)
test_data = lgb.Dataset(x_test, label=y_test_encoded, reference=train_data)

# Set the parameters for LightGBM
params = {
    'objective': 'binary',
    'metric': 'binary_logloss',
    'boosting_type': 'gbdt',
    'num_leaves': 10,
    'learning_rate': 0.05,
    'verbose':-1,
    'seed': SEED,
}

evals_result = {}

# Train the model
num_round = 200
bst = lgb.train(
    params,
    train_data,
    num_round,
    valid_sets=[train_data, test_data],
    callbacks=[lgb.log_evaluation(10), lgb.record_evaluation(evals_result)]
    )

# Make predictions
y_pred_lgbm = bst.predict(x_test, num_iteration=bst.best_iteration)

# Convert probabilities to binary predictions
y_pred_binary = np.where(y_pred_lgbm > 0.5, 1, 0)

end_lgbm = time.time()

print(f'Runtime for Light GBM: {round(end_lgbm - start_lgbm)} seconds')

lgb.plot_importance(bst, importance_type='auto', max_num_features=15, ignore_zero=True)
plt.show()

# Evaluate the model
accuracy_score(y_test_encoded, y_pred_binary)

print('Light GBM')
print(classification_report(y_test_encoded, y_pred_binary))

# Original confusion matrix
original_cm = confusion_matrix(y_test_encoded, y_pred_binary)

desired_order = [1, 0]  # this will reverse the order

# Create a mapping from original labels to desired order
mapping = {label: i for i, label in enumerate(desired_order)}

# Reorder confusion matrix
reordered_cm = original_cm[np.ix_(desired_order, desired_order)]

# Plot the reordered confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=reordered_cm, display_labels=labels)
disp.plot(cmap='Blues')
plt.title('Light GBM')
plt.show()

lgb.plot_tree(bst, tree_index=100)

