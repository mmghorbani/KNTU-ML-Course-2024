# -*- coding: utf-8 -*-
"""ML_Midterm_Q4_Ghorbani_40110424.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e3alND1DYCsXw1UBpKXEjo0dzsM4wOUM

# Q4
Midterm Exam

# Part I (Train)
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import scipy.io as sio
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
import seaborn as sns
import random
from sklearn.metrics import classification_report

import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense

!pip install --upgrade --no-cach-dir gdown
! gdown 1u4gZrRlK2GD4x0MMt66_cvgNt4ZCcxhi

dataset = sio.loadmat('/content/DATA.mat')
data = pd.DataFrame(dataset['NOV9'])

# Remove index coloumn
data = data.loc[:,data.columns!=0]

data

# Extract Fault Time Ranges
data_normal = data.loc[0:57270, :]

fault_16 = data.loc[57275:57550, :]

df_18_1 = data.loc[58830:589300, :]
df_18_2 = data.loc[58520:58625, :]
fault_18 = pd.concat([df_18_1, df_18_2])

print(len(data_normal), len(fault_16), len(fault_18))

len(data.columns)

M = 5
N = 50

X = []  # Initialize an empty list for features
Y = []  # Initialize an empty list for labels
H = []

# Normal class
for i in range(M*100):
    start_idx = i * N
    end_idx = start_idx + N
    x = np.array(data_normal.iloc[start_idx:end_idx, 1])
    X.append(x)  # Append each feature vector to X
    Y.append(0)  # Append the label (0 for normal class)

for j in range(M*100):
    start_idx = j * N
    end_idx = start_idx + N
    x = np.array(fault_18.iloc[start_idx:end_idx, 1])
    X.append(x)  # Append each feature vector to X
    Y.append(1)  # Append the label (1 for fault 18)

for j in range(M):
    start_idx = j * N
    end_idx = start_idx + N
    x = np.array(fault_16.iloc[start_idx:end_idx, 1])
    X.append(x)  # Append each feature vector to X
    Y.append(2)  # Append the label (2 for fault 16)


# Convert lists to numpy arrays
X = np.array(X)
Y = np.array(Y).reshape(-1, 1)

print(X.shape, Y.shape)

# Square Mean Root
def smr(x):
  return np.mean(np.sqrt(abs(x)))**2

# Root Mean Square
def rms(x):
  return np.sqrt(np.mean((x**2)))

# Absolut Mean
def abs_mean(x):
  return np.mean(abs(x))

# Impact Factor 1
def IF1(x):
  return np.max(x) / (abs_mean(x))

def feature_extract(x):
  y = np.zeros((1,8))
  y[0,0] = np.mean(x)
  y[0,1] = np.std(x)
  y[0,2] = np.max(x)
  y[0,3] = np.max(x) - np.min(x)
  y[0,4] = smr(x)
  y[0,5] = rms(x)
  y[0,6] = abs_mean(x)
  y[0,7] = IF1(x)
  return y

Xnew = np.zeros(shape = (len(X),8))

for i in range(len(X)):
  Xnew[[i],:] = feature_extract(X[[i],:])

# Shuffle both X and Y with same order
X_shuffled, Y_shuffled = shuffle(Xnew,Y, random_state = 24)

x_train , x_val , y_train , y_val = train_test_split(X_shuffled, Y_shuffled, test_size = 0.2, random_state = 24)
x_train.shape, x_val.shape, y_train.shape, y_val.shape

scaler = MinMaxScaler()
scaler.fit(x_train)

x_train_normalized = scaler.transform(x_train)
x_val_normalized = scaler.transform(x_val)

print(np.min(x_train_normalized), np.min(x_val_normalized))
print(np.max(x_train_normalized), np.max(x_val_normalized))

plt.rcParams["figure.figsize"] = (15, 5)

plt.subplot(1,2,1)
plt.hist(x_train_normalized[:,0])
plt.title('Histplot of mean values for Training set')

plt.subplot(1,2,2)
plt.hist(x_train_normalized[:,0])
plt.title('Histplot of mean values for Validation set')

tf.random.set_seed(24)
random.seed(24)

model_1 = Sequential([
    Dense(units = 10, activation='relu'),
    Dense(units = 7, activation='relu'),
    Dense(units = 3, activation='softmax'),
])

model_1.compile(
    optimizer='Adam',
    loss='SparseCategoricalCrossentropy',
    metrics=['accuracy']
    )

hist_1 = model_1.fit(
    x=x_train_normalized,
    y=y_train,
    validation_data=(x_val_normalized,y_val),
    epochs=100,
    batch_size=10,
    verbose=2,
    )

# Plot the training and validation loss
plt.rcParams["figure.figsize"] = (12, 4)
plt.plot(hist_1.history['loss'], color='blue', linewidth=2)   # Training loss
plt.plot(hist_1.history['val_loss'], color='red', linewidth=2)  # Validation loss

plt.legend(['Training Loss', 'Validation Loss'])
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title('Optimizer = Adam, Loss = Sparse Categorical Crossentropy')
plt.grid(True, linewidth = 1, linestyle=':')
plt.show()

tf.random.set_seed(24)
random.seed(24)

model_2 = Sequential([
    Dense(units = 10, activation='relu'),
    Dense(units = 5, activation='relu'),
    Dense(units = 4, activation='softmax'),
])

model_2.compile(
    optimizer='SGD',
    loss='KLDivergence',
    metrics=['accuracy']
    )

hist_2 = model_2.fit(
    x=x_train_normalized,
    y=y_train,
    validation_data=(x_val_normalized,y_val),
    epochs=100,
    batch_size=10,
    verbose=2,
    )

# Plot the training and validation loss
plt.rcParams["figure.figsize"] = (12, 4)
plt.plot(hist_2.history['loss'], color='blue', linewidth=2)
plt.plot(hist_2.history['val_loss'], color='red', linewidth=2)

plt.legend(['Training Loss', 'Validation Loss'])
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title('Optimizer = SGD, Loss = Kullback Leibler Divergence')
plt.grid(True, linewidth = 1, linestyle=':')
plt.show()

"""# Part II (Test)"""

data_test = pd.DataFrame(dataset['NOV17'])

# Remove index coloumn
data_test = data_test.loc[:, data_test.columns!=0]

data_test

# Extract Fault Time Ranges
normal_test = data_test.loc[0:5460, :]
fault_16_test = data_test.loc[56670:56770, :]
fault_18_test = data_test.loc[54600:54700, :]

print(len(normal_test), len(fault_16_test), len(fault_18_test))

M = 2
N = 40

X_test = []  # Initialize an empty list for features
Y_test = []  # Initialize an empty list for labels

# Normal class
for i in range(200):
    start_idx = i * N
    end_idx = start_idx + N
    x = np.array(normal_test.iloc[start_idx:end_idx, 1])
    X_test.append(x)  # Append each feature vector to X
    Y_test.append(0)  # Append the label (0 for normal class)

for j in range(M):
    start_idx = j * N
    end_idx = start_idx + N
    x = np.array(fault_18_test.iloc[start_idx:end_idx, 1])
    X_test.append(x)  # Append each feature vector to X
    Y_test.append(1)  # Append the label (1 for fault 18)

for h in range(M):
    start_idx = h * N
    end_idx = start_idx + N
    x = np.array(fault_16_test.iloc[start_idx:end_idx, 1])
    X_test.append(x)  # Append each feature vector to X
    Y_test.append(2)  # Append the label (2 for fault 16)