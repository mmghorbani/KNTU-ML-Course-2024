# -*- coding: utf-8 -*-
"""ML_MP3_Q3_Ghorbani.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XNNX6GqfQQNicw0piSzyAlc7fYryO2AU
"""
# Q3

# Part I
# Preprocessing data


# Import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sb
from collections import Counter
import random
import os

from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, accuracy_score

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, GaussianNoise
from tensorflow.keras.callbacks import ModelCheckpoint

# Import dataset
!pip install --upgrade --no-cach-dir gdown
!gdown 121ms5G2e4cXG01A4Os8C64AOomlM3Xhj

# Set random seed
SEED = 24
os.environ['PYTHONHASHSEED'] = str(SEED)
os.environ['TF_DETERMINISTIC_OPS'] = '1'
np.random.seed(SEED)
random.seed(SEED)
tf.random.set_seed(SEED)

# Load dataset
dataset = pd.read_csv('/content/creditcard.csv')
dataset = dataset.dropna() # Drop nulls
dataset.head()

# Normalize "Amount" feature
print(np.min(dataset['Amount']), np.max(dataset['Amount']))

def min_max_scaling(X):
    X_min = np.min(X)
    X_max = np.max(X)
    X_normalized = (X - X_min) / (X_max - X_min)
    return X_normalized

dataset['Amount'] = min_max_scaling(dataset['Amount'])
print(np.min(dataset['Amount']), np.max(dataset['Amount']))

# Extract feature matrix and target vector
X = dataset.iloc[:,1:-1]
y = dataset['Class']

# Split data to train, test, validation with ratios of 70%, 15%, 15% respectively
X_train, X_temp, y_train, y_temp = train_test_split(X.values, y.values, test_size=0.3, random_state=SEED)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=SEED)

# Apply SMOTE to the training data
sm = SMOTE(sampling_strategy='minority', random_state=SEED)

X_train_res, y_train_res = sm.fit_resample(X_train, y_train)

print(f"Labeles befor oversampling: {Counter(y_train)}")
print(f"Labeles After oversampling: {Counter(y_train_res)}")

# Add Gaussian noise to the resampled training data
noise_factor = 0.5
X_train_noisy = X_train_res + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=X_train_res.shape)

"""# Part II
Train and evaluation without sliding oversampling threshold
"""

DAE = Sequential([
    Dense(22, activation='leaky_relu', input_shape=(29,)),
    Dense(15, activation='leaky_relu'),
    Dense(10, activation='leaky_relu'),
    Dense(15, activation='leaky_relu'),
    Dense(22, activation='leaky_relu'),
    Dense(29, activation='leaky_relu')
])

DAE.compile(optimizer='adam', loss='mse', metrics=['accuracy'])

DAE_checkpoint = ModelCheckpoint('best_DAE.keras', monitor='val_loss', save_best_only=True, mode='min')

DAE_histoy = DAE.fit(
    X_train_noisy,
    X_train_res,
    epochs=10,
    batch_size=64,
    validation_data=(X_val, X_val),
    callbacks=[DAE_checkpoint],
    verbose=2
    )

# Load the best weights for DAE
DAE.load_weights('best_DAE.keras')

plt.rcParams['figure.dpi'] = 120
plt.plot(DAE_histoy.history['loss'], color='blue', linewidth=2)   # Training loss
plt.plot(DAE_histoy.history['val_loss'], color='red', linewidth=2)  # Validation loss

plt.legend(['Training Loss', 'Validation Loss'])
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.suptitle('Denoising Autoencode')
plt.title('Optimizer = Adam, Loss = MSE')
plt.grid(True, linewidth = 1, linestyle=':')
plt.show()

# Prepare denoised data for training the classifier
X_train_denoised = DAE.predict(X_train_noisy)
X_val_denoised = DAE.predict(X_val)

# Define the classifier model
clf = Sequential([
    Dense(22, activation='leaky_relu', input_shape=(29,)),
    Dense(15, activation='leaky_relu'),
    Dense(10, activation='leaky_relu'),
    Dense(5, activation='leaky_relu'),
    Dense(2, activation='sigmoid')
    ])

clf.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the classifier
clf_checkpoint = ModelCheckpoint('best_classifier.keras', monitor='val_loss', save_best_only=True, mode='min')

clf_history = clf.fit(
    X_train_denoised,
    y_train_res,
    epochs=10,
    batch_size=64,
    validation_data=(X_val_denoised, y_val),
    callbacks=[clf_checkpoint],
    verbose=2
    )

# Load the best weights for classifier
clf.load_weights('best_classifier.keras')

plt.plot(clf_history.history['loss'], color='blue', linewidth=2)   # Training loss
plt.plot(clf_history.history['val_loss'], color='red', linewidth=2)  # Validation loss

plt.legend(['Training Loss', 'Validation Loss'])
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.suptitle('Classifier')
plt.title('Optimizer = Adam, Loss = Crossentropy')
plt.grid(True, linewidth = 1, linestyle=':')
plt.show()

# Evlation
X_test_denoised = DAE.predict(X_test)
y_prob = clf.predict(X_test_denoised)
y_pred = np.argmax(y_prob, axis=1)

# Metrics
print(f'\n Accuracy = {accuracy_score(y_test, y_pred):.2f} \n Recall = {recall_score(y_test, y_pred):.2f} \n Precision = {precision_score(y_test, y_pred):.2f} \n F1-Score = {f1_score(y_test, y_pred):.2f} ')

# Plot confusion matrix
cm1 = confusion_matrix(y_test, y_pred)
sb.heatmap(cm1, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

"""# Part III
Train and evaluation with sliding oversampling threshold
"""

threshold = np.arange(0.1, 1, 0.1)
acc = []
rec = []

for i in threshold:
  print(f'Threshold = {i}')
  sm = SMOTE(sampling_strategy=i, random_state=SEED)
  X_train_res, y_train_res = sm.fit_resample(X_train, y_train)
  X_train_noisy = X_train_res + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=X_train_res.shape)

  DAE_checkpoint = ModelCheckpoint('best_DAE.keras', monitor='val_loss', save_best_only=True, mode='min')
  DAE_histoy = DAE.fit(
    X_train_noisy,
    X_train_res,
    epochs=10,
    batch_size=64,
    validation_data=(X_val, X_val),
    callbacks=[DAE_checkpoint],
    verbose=0
    )
  DAE.load_weights('best_DAE.keras')

  # Prepare denoised data for training the classifier
  X_train_denoised = DAE.predict(X_train_noisy)
  X_val_denoised = DAE.predict(X_val)

  # Train the classifier
  clf_checkpoint = ModelCheckpoint('best_classifier.keras', monitor='val_loss', save_best_only=True, mode='min')
  clf_history = clf.fit(
    X_train_denoised,
    y_train_res,
    epochs=10,
    batch_size=64,
    validation_data=(X_val_denoised, y_val),
    callbacks=[clf_checkpoint],
    verbose=0
    )
  clf.load_weights('best_classifier.keras')

  X_test_denoised = DAE.predict(X_test)
  y_prob = clf.predict(X_test_denoised)
  y_pred = np.argmax(y_prob, axis=1)

  acc.append(accuracy_score(y_test, y_pred))
  rec.append(recall_score(y_test, y_pred))

plt.plot(threshold, rec, label='Recall')
plt.plot(threshold, acc, label='Accuracy')
plt.ylim(0,1.1)
plt.title('Sliding sampling strategy in oversampling')
plt.xlabel('Threshold 0~1')
plt.ylabel('Score')
plt.legend()

"""# Part IV
Train and evaluation without oversampling
"""

# Train the classifier
clf_checkpoint = ModelCheckpoint('best_classifier.keras', monitor='val_loss', save_best_only=True, mode='min')
clf_history = clf.fit(
    X_train,
    y_train,
    epochs=10,
    batch_size=64,
    validation_data=(X_val, y_val),
    callbacks=[clf_checkpoint],
    verbose=2
    )
clf.load_weights('best_classifier.keras')

# Evlation
y_prob = clf.predict(X_test)
y_pred = np.argmax(y_prob, axis=1)

# Metrics
print(f'\n Accuracy = {accuracy_score(y_test, y_pred):.2f} \n Recall = {recall_score(y_test, y_pred):.2f} \n Precision = {precision_score(y_test, y_pred):.2f} \n F1-Score = {f1_score(y_test, y_pred):.2f} ')

# Plot confusion matrix
cm2 = confusion_matrix(y_test, y_pred)
sb.heatmap(cm2, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix (without oversampling)')
plt.show()

