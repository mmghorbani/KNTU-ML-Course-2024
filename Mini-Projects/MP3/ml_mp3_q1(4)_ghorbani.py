# -*- coding: utf-8 -*-
"""ML_MP3_Q1(4)_Ghorbani.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RoyU03iCXIotwvpnr9XfVD9ljcJoZuuu
"""
# Q1(4)
# This code implements a machine learning pipeline for classifying the Iris dataset using a custom Support Vector Machine (SVM) with a polynomial kernel. The key steps include loading and preprocessing the dataset with Linear Discriminant Analysis (LDA) for dimensionality reduction, splitting the data into training and testing sets, and standardizing the features. The SVM is trained using a one-vs-one strategy for multi-class classification, and a custom quadratic programming solver is employed for optimization. The model's performance is evaluated across different polynomial degrees, with accuracy and F1-score metrics. Additionally, decision boundaries are visualized and saved as a GIF to illustrate the classification results across various polynomial degrees.
"""
# Part IV
"""

# Import libraries
import numpy as np
from cvxopt import matrix, solvers
import matplotlib.pyplot as plt
from itertools import combinations
import imageio.v2 as iio

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
from sklearn.metrics import accuracy_score, f1_score

# Load iris dataset
iris = load_iris()
X = iris.data
y = iris.target

# Perform LDA to reduce to 2 components
lda = LDA(n_components=2)
X_lda = lda.fit_transform(X, y)

# Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X_lda, y, test_size=0.2, random_state=24)

# Standardize features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Figure parameters
plt.rcParams['figure.figsize'] = (8, 5)
plt.rcParams['figure.dpi'] = 120

# Define SVM class from scratch
class SVM:
    def __init__(self, degree=3, C=None):
        self.degree = degree
        self.C = C
        self.models = []
        self.classes = []

    def polynomial_kernel(self, X, Y=None):
        if Y is None:
            Y = X
        return (1 + np.dot(X, Y.T)) ** self.degree

    def fit(self, X, y):
        self.classes = np.unique(y)
        n_samples, n_features = X.shape

        # Train one-vs-one SVMs
        for (i, j) in combinations(self.classes, 2):
            mask = (y == i) | (y == j)
            X_ij = X[mask]
            y_ij = y[mask]
            y_ij = np.where(y_ij == i, 1, -1)

            K = self.polynomial_kernel(X_ij)
            P = matrix(np.outer(y_ij, y_ij) * K + np.eye(len(y_ij)) * 1e-2)  # Increased regularization term
            q = matrix(-np.ones(len(y_ij)))
            A = matrix(y_ij, (1, len(y_ij)), 'd')
            b = matrix(0.0)

            if self.C is None:
                G = matrix(-np.eye(len(y_ij)))
                h = matrix(np.zeros(len(y_ij)))
            else:
                G = matrix(np.vstack((-np.eye(len(y_ij)), np.eye(len(y_ij)))))
                h = matrix(np.hstack((np.zeros(len(y_ij)), np.ones(len(y_ij)) * self.C)))

            solvers.options['show_progress'] = False
            sol = solvers.qp(P, q, G, h, A, b)
            alpha = np.ravel(sol['x'])

            sv = alpha > 1e-5
            if np.sum(sv) == 0:
                print(f"Warning: No support vectors found for class pair ({i}, {j}). Skipping.")
                continue

            ind = np.arange(len(alpha))[sv]
            alpha = alpha[sv]
            sv_X = X_ij[sv]
            sv_y = y_ij[sv]

            b = 0
            for n in range(len(alpha)):
                b += sv_y[n]
                b -= np.sum(alpha * sv_y * K[ind[n], sv])
            b /= len(alpha)

            self.models.append((alpha, sv_X, sv_y, b))

    def project(self, X):
        y_pred = np.zeros((X.shape[0], len(self.models)))
        for i, (alpha, sv, sv_y, b) in enumerate(self.models):
            y_pred[:, i] = np.sum(alpha * sv_y * self.polynomial_kernel(X, sv), axis=1) + b
        return y_pred

    def predict(self, X):
        y_pred = self.project(X)
        votes = np.zeros((X.shape[0], len(self.classes)))
        k = 0
        for i, j in combinations(range(len(self.classes)), 2):
            votes[:, i] += (y_pred[:, k] > 0).astype(int)
            votes[:, j] += (y_pred[:, k] < 0).astype(int)
            k += 1
        return self.classes[np.argmax(votes, axis=1)]

    def plot_decision_boundary(self, X, y):
        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
        xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),
                             np.arange(y_min, y_max, 0.02))
        Z = self.predict(np.c_[xx.ravel(), yy.ravel()])
        Z = Z.reshape(xx.shape)

        fig = plt.figure()
        plt.contourf(xx, yy, Z, alpha=0.8)
        plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', marker='o')
        plt.title(f"SVM with polynomial kernel from Scratch (degree = {self.degree}), LDA")
        return fig

images = []

for i in range(1,11):
  # Define the model
  clf = SVM(degree=i)

  # Train model
  clf.fit(X_train, y_train)

  # Prediction
  y_pred = clf.predict(X_test)

  # Print accuracy and f1-score
  print(f'Degree = {i}, accuracy = {accuracy_score(y_test,  y_pred)* 100:.2f}, f1-score = {f1_score(y_test,  y_pred, average="macro"):.2f}')

  # Plot decision boundary
  clf.plot_decision_boundary(X_train, y_train)
  plt.savefig(f'scratch_poly_degree_{i}.png')
  images.append(iio.imread((f'scratch_poly_degree_{i}.png')))
  plt.close('all')

iio.mimsave('scratch_decision_boundries.gif', images, duration=1000, loop=0)

